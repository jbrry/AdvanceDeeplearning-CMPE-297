{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4g_BERT_MTL_Framework.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPTB5g+34mKH2OGTE23UclA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-c-soma/AdvanceDeeplearning-CMPE-297/blob/master/Assignment_4/Assignment_4g_BERT_MTL_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7YPA59dqKir"
      },
      "source": [
        "# BERT MTL framework\n",
        "\n",
        "Asignment 4g\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjnElINdrf5S"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yL0oZeYrJuq"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from bert_multitask_learning import (get_or_make_label_encoder, FullTokenizer, \n",
        "                                     create_single_problem_generator, train_bert_multitask, \n",
        "                                     eval_bert_multitask, DynamicBatchSizeParams, TRAIN, EVAL, PREDICT, BertMultiTask,preprocessing_fn)\n",
        "import pickle\n",
        "import types\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnRPtGsGrJut",
        "outputId": "64efaa20-8039-4683-9198-2de25c44878a"
      },
      "source": [
        "cd ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/data3/yjp/bert-multitask-learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XasPN95IrJuw"
      },
      "source": [
        "# define new problem\n",
        "new_problem_type = {'imdb_cls': 'cls'}\n",
        "\n",
        "@preprocessing_fn\n",
        "def imdb_cls(params, mode):\n",
        "    # get data\n",
        "    (train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=10000)\n",
        "    label_encoder = get_or_make_label_encoder(params, 'imdb_cls', mode, train_labels+test_labels)\n",
        "    word_to_id = keras.datasets.imdb.get_word_index()\n",
        "    index_from=3\n",
        "    word_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\n",
        "    word_to_id[\"<PAD>\"] = 0\n",
        "    word_to_id[\"<START>\"] = 1\n",
        "    word_to_id[\"<UNK>\"] = 2\n",
        "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "\n",
        "    train_data = [[id_to_word[i] for i in sentence] for sentence in train_data]\n",
        "    test_data = [[id_to_word[i] for i in sentence] for sentence in test_data]\n",
        "    \n",
        "    if mode == TRAIN:\n",
        "        input_list = train_data\n",
        "        target_list = train_labels\n",
        "    else:\n",
        "        input_list = test_data\n",
        "        target_list = test_labels\n",
        "    \n",
        "    return input_list, target_list\n",
        "new_problem_process_fn_dict = {'imdb_cls': imdb_cls}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjKEyjjirJuy"
      },
      "source": [
        "# create params and model\n",
        "params = DynamicBatchSizeParams()\n",
        "params.init_checkpoint = 'models/cased_L-12_H-768_A-12'\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "model = BertMultiTask(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nxIa2c9rJup"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "By default, the model only contains BERT model and a dense layer for each problem. If you want to add things between BERT and dense layers, you can modify hidden method of BertMultiTask class. Here's an example of adding a cudnn GRU on top of BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYC38Ci8rJu0"
      },
      "source": [
        "def cudnngru_hidden(self, features, hidden_feature, mode):\n",
        "    # with shape (batch_size, seq_len, hidden_size)\n",
        "    seq_hidden_feature = hidden_feature['seq']\n",
        "    \n",
        "    cudnn_gru_layer = tf.keras.layers.CuDNNGRU(\n",
        "            units=self.params.bert_config.hidden_size,\n",
        "            return_sequences=True,\n",
        "            return_state=False,\n",
        "    )\n",
        "    gru_logit = cudnn_gru_layer(seq_hidden_feature)\n",
        "    \n",
        "    return_features = {}\n",
        "    return_hidden_feature = {}\n",
        "    \n",
        "    for problem_dict in self.params.run_problem_list:\n",
        "        for problem in problem_dict:\n",
        "            # for slightly faster training\n",
        "            return_features[problem], return_hidden_feature[problem] = self.get_features_for_problem(\n",
        "                    features, hidden_feature, problem, mode)\n",
        "    return return_features, return_hidden_feature\n",
        "\n",
        "model.hidden = types.MethodType(cudnngru_hidden, model)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezxtrqrlrlhs"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2jpKaQ4rJu2",
        "outputId": "2e1fb3a1-5c33-46c0-a9ee-cb789fac24f2"
      },
      "source": [
        "# train model\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "train_bert_multitask(problem='imdb_cls', num_gpus=1, \n",
        "                     num_epochs=10, params=params, \n",
        "                     problem_type_dict=new_problem_type, processing_fn_dict=new_problem_process_fn_dict, \n",
        "                     model=model, model_dir='models/ibdm_gru')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding new problem imdb_cls, problem type: cls\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:1\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:2\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:3\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:1\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:2\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:3\n",
            "INFO:tensorflow:Configured nccl all-reduce.\n",
            "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'models/ibdm_gru', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f1ac3d61828>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f1ac3d61828>, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1ac3d61518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
            "INFO:tensorflow:Create RestoreCheckpointHook.\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f1ac3d61780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqS24Jnnrp_i"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U60LLnOrJu5",
        "outputId": "9cd5081a-92cc-4549-bcc0-4ca2c285524a"
      },
      "source": [
        "# evaluate model\n",
        "print(eval_bert_multitask(problem='imdb_cls', num_gpus=1, \n",
        "                     params=params, eval_scheme='acc',\n",
        "                     problem_type_dict=new_problem_type, processing_fn_dict=new_problem_process_fn_dict,\n",
        "                     model_dir='models/idbm_gru', model = model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params problem assigned. Problem list: ['imdb_cls']\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:1\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:2\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:3\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:1\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:2\n",
            "INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:3\n",
            "INFO:tensorflow:Configured nccl all-reduce.\n",
            "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'models/ibdm_gru', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f1acbec6668>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f1acbec6668>, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1acbec6400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
            "WARNING:tensorflow:From /data3/yjp/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /data3/yjp/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /data3/yjp/bert-multitask-learning/bert_multitask_learning/bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "DEBUG:tensorflow:Converted call: <function stop_grad at 0x7f1acc1c6ea0>; owner: None\n",
            "DEBUG:tensorflow:Converting <function stop_grad at 0x7f1acc1c6ea0>\n",
            "DEBUG:tensorflow:Compiled output of <function stop_grad at 0x7f1acc1c6ea0>:\n",
            "\n",
            "def stop_grad(global_step, tensor, freeze_step):\n",
            "  try:\n",
            "    with ag__.function_scope('stop_grad'):\n",
            "      cond_1 = ag__.gt(freeze_step, 0)\n",
            "\n",
            "      def if_true_1():\n",
            "        with ag__.function_scope('if_true_1'):\n",
            "          tensor_2, = tensor,\n",
            "          cond = ag__.lt_e(global_step, freeze_step)\n",
            "\n",
            "          def if_true():\n",
            "            with ag__.function_scope('if_true'):\n",
            "              tensor_1, = tensor_2,\n",
            "              tensor_1 = tf.stop_gradient(tensor_1)\n",
            "              return tensor_1\n",
            "\n",
            "          def if_false():\n",
            "            with ag__.function_scope('if_false'):\n",
            "              return tensor_2\n",
            "          tensor_2 = ag__.if_stmt(cond, if_true, if_false)\n",
            "          return tensor_2\n",
            "\n",
            "      def if_false_1():\n",
            "        with ag__.function_scope('if_false_1'):\n",
            "          return tensor\n",
            "      tensor = ag__.if_stmt(cond_1, if_true_1, if_false_1)\n",
            "      return tensor\n",
            "  except:\n",
            "    ag__.rewrite_graph_construction_error(ag_source_map__)\n",
            "\n",
            "\n",
            "\n",
            "stop_grad.autograph_info__ = {}\n",
            "\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /data3/yjp/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from models/ibdm_gru/model.ckpt-7812\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing Inputs: 100%|██████████| 25000/25000 [05:08<00:00, 81.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'imdb_cls_Accuracy': 0.91332, 'imdb_cls_Accuracy Per Sequence': 0.91332}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}